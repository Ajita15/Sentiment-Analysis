{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#preprocessing parts\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense,Dropout,Input,Embedding,LSTM\n",
    "from keras.models import Sequential,Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.initializers import Constant\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix,f1_score,recall_score,precision_score\n",
    "from keras.layers import Conv1D,MaxPooling1D,GlobalMaxPooling1D\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import inflect\n",
    "import string\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix,f1_score,recall_score,precision_score,roc_curve,roc_auc_score\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5693, 6)\n",
      "Index(['id', 'App Version Code', 'App Version Name', 'Review Text',\n",
      "       'Review Title', 'Star Rating'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df1=pd.read_csv(\"train.csv\")\n",
    "print(df1.shape)\n",
    "print(df1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1424, 5)\n",
      "Index(['id', 'App Version Code', 'App Version Name', 'Review Text',\n",
      "       'Review Title'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df2=pd.read_csv(\"test.csv\")\n",
    "print(df2.shape)\n",
    "print(df2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>App Version Code</th>\n",
       "      <th>App Version Name</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Review Title</th>\n",
       "      <th>Star Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b817b0f0-a2f8-4c9d-a5f6-d3fbf8b1d7e9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Very bad wallet balance not use.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c7844e8f-56c1-487b-ae3e-df2fdf4c1767</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2.30</td>\n",
       "      <td>Froud app i recharge 199 but not done also sen...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>93ff57f7-9e02-4fa9-b779-3db82b8af0a4</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2.12</td>\n",
       "      <td>Waste to write comments also</td>\n",
       "      <td>Waste no use</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>df2dcdef-c09a-4f35-afab-e1231d3fec9a</td>\n",
       "      <td>62.0</td>\n",
       "      <td>2.21</td>\n",
       "      <td>Nice apo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11f8f968-4cec-4424-8427-9709ab05b5be</td>\n",
       "      <td>69.0</td>\n",
       "      <td>2.28</td>\n",
       "      <td>Good nice app</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  App Version Code  App Version Name  \\\n",
       "0  b817b0f0-a2f8-4c9d-a5f6-d3fbf8b1d7e9               NaN               NaN   \n",
       "1  c7844e8f-56c1-487b-ae3e-df2fdf4c1767              43.0              2.30   \n",
       "2  93ff57f7-9e02-4fa9-b779-3db82b8af0a4              52.0              2.12   \n",
       "3  df2dcdef-c09a-4f35-afab-e1231d3fec9a              62.0              2.21   \n",
       "4  11f8f968-4cec-4424-8427-9709ab05b5be              69.0              2.28   \n",
       "\n",
       "                                         Review Text  Review Title  \\\n",
       "0                   Very bad wallet balance not use.           NaN   \n",
       "1  Froud app i recharge 199 but not done also sen...           NaN   \n",
       "2                       Waste to write comments also  Waste no use   \n",
       "3                                           Nice apo           NaN   \n",
       "4                                      Good nice app           NaN   \n",
       "\n",
       "   Star Rating  \n",
       "0            1  \n",
       "1            1  \n",
       "2            2  \n",
       "3            5  \n",
       "4            5  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                     0\n",
       "App Version Code    1139\n",
       "App Version Name    1139\n",
       "Review Text            1\n",
       "Review Title        5091\n",
       "Star Rating            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(df1.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                     0\n",
       "App Version Code     273\n",
       "App Version Name     273\n",
       "Review Text            1\n",
       "Review Title        1244\n",
       "dtype: int64"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(df2.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df2.dropna(axis=0, subset=['Review Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df1.dropna(axis=0, subset=['Review Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Review Title']=df1['Review Title'].replace(np.nan,df1['Review Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                     0\n",
       "App Version Code    1138\n",
       "App Version Name    1138\n",
       "Review Text            0\n",
       "Review Title           0\n",
       "Star Rating            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(df1.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen_doc=max(len(docs) for docs in df1['Review Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1816"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "apostropheList = {\"n't\" : \"not\",\"aren't\" : \"are not\",\"can't\" : \"cannot\",\"couldn't\" : \"could not\",\"didn't\" : \"did not\",\"doesn't\" : \"does not\", \\\n",
    "                  \"don't\" : \"do not\",\"hadn't\" : \"had not\",\"hasn't\" : \"has not\",\"haven't\" : \"have not\",\"he'd\" : \"he had\",\"he'll\" : \"he will\", \\\n",
    "                  \"he's\" : \"he is\",\"I'd\" : \"I had\",\"I'll\" : \"I will\",\"I'm\" : \"I am\",\"I've\" : \"I have\",\"isn't\" : \"is not\",\"it's\" : \\\n",
    "                  \"it is\",\"let's\" : \"let us\",\"mustn't\" : \"must not\",\"shan't\" : \"shall not\",\"she'd\" : \"she had\",\"she'll\" : \"she will\", \\\n",
    "                  \"she's\" : \"she is\", \"shouldn't\" : \"should not\",\"that's\" : \"that is\",\"there's\" : \"there is\",\"they'd\" : \"they had\", \\\n",
    "                  \"they'll\" : \"they will\", \"they're\" : \"they are\",\"they've\" : \"they have\",\"we'd\" : \"we had\",\"we're\" : \"we are\",\"we've\" : \"we have\", \\\n",
    "                  \"weren't\" : \"were not\", \"what'll\" : \"what will\",\"what're\" : \"what are\",\"what's\" : \"what is\",\"what've\" : \"what have\", \\\n",
    "                  \"where's\" : \"where is\",\"who'd\" : \"who had\", \"who'll\" : \"who will\",\"who're\" : \"who are\",\"who's\" : \"who is\",\"who've\" : \"who have\", \\\n",
    "                    \"won't\" : \"will not\",\"wouldn't\" : \"would not\", \"you'd\" : \"you had\",\"you'll\" : \"you will\",\"you're\" : \"you are\",\"you've\" : \"you have\",\\\n",
    "                    \"It's\": \"It is\"}\n",
    "\n",
    "lemm=WordNetLemmatizer()\n",
    "p=inflect.engine()\n",
    "def remove_punctuation(sent):\n",
    "    #removes punctuation \n",
    "    punct=string.punctuation\n",
    "    l=[]\n",
    "    for chrs in sent:\n",
    "        if chrs not in punct:\n",
    "            l.append(chrs)\n",
    "    return ''.join(l)\n",
    "\n",
    "def lower_numbers(sent):\n",
    "    #lower the words and converts numbers to words like 2-two\n",
    "    l=[]\n",
    "    for word in sent.strip().split():\n",
    "        if word.isdigit():\n",
    "            try:\n",
    "                l.append(p.number_to_words(word))\n",
    "            except:\n",
    "                l.append('')\n",
    "        else:\n",
    "            l.append(word.lower())\n",
    "    return ' '.join(l)\n",
    "\n",
    "def remove_stopwords(sent):\n",
    "    stpwrds=stopwords.words('english')\n",
    "    l=[word for word in sent.strip().split() if word not in stpwrds]\n",
    "    return ' '.join(l)\n",
    "\n",
    "def remove_non_ascii(sent):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in sent.strip().split():\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "def lemmatize(sent):\n",
    "    #lemmatizes the tokens\n",
    "    words=word_tokenize(sent)\n",
    "    l=[]\n",
    "    for word in words:\n",
    "        l.append(lemm.lemmatize(word))\n",
    "    return ' '.join(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(data):\n",
    "    text_list=[]\n",
    "    for texts in data:\n",
    "        try:\n",
    "            doc=remove_non_ascii(texts)\n",
    "            doc=remove_punctuation(doc)\n",
    "            doc=lower_numbers(doc)\n",
    "            doc=remove_stopwords(doc)\n",
    "            doc=lemmatize(doc)\n",
    "        except AttributeError:\n",
    "            doc=''\n",
    "        text_list.append(doc)\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=[]\n",
    "for doc in df1['Review Text']:\n",
    "    doc=doc.split()\n",
    "    text=[]\n",
    "    for word in doc:\n",
    "        if word in apostropheList.keys():\n",
    "            text.append(apostropheList[word])\n",
    "        else:\n",
    "            text.append(word)\n",
    "    #doc=[apostropheList[word] for word in doc if word in apostropheList.keys() else ]\n",
    "    texts.append(\" \".join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=text_preprocessing(texts)\n",
    "df1['cleaned review text']=texts\n",
    "texts=[]\n",
    "for doc in df1['Review Title']:\n",
    "    doc=doc.split()\n",
    "    text=[]\n",
    "    for word in doc:\n",
    "        if word in apostropheList.keys():\n",
    "            text.append(apostropheList[word])\n",
    "        else:\n",
    "            text.append(word)\n",
    "    #doc=[apostropheList[word] for word in doc if word in apostropheList.keys() else ]\n",
    "    texts.append(\" \".join(text))\n",
    "texts=text_preprocessing(texts)\n",
    "df1['cleaned review title']=texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=[]\n",
    "for doc in df2['Review Text']:\n",
    "    doc=doc.split()\n",
    "    text=[]\n",
    "    for word in doc:\n",
    "        if word in apostropheList.keys():\n",
    "            text.append(apostropheList[word])\n",
    "        else:\n",
    "            text.append(word)\n",
    "    #doc=[apostropheList[word] for word in doc if word in apostropheList.keys() else ]\n",
    "    texts.append(\" \".join(text))\n",
    "texts=text_preprocessing(texts)\n",
    "df2['cleaned review text']=texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_texts=[len(doc) for doc in df1['cleaned review text']]\n",
    "len_title=[len(doc) for doc in df1['cleaned review title']]\n",
    "df1['len title']=len_title\n",
    "df1['len texts']=len_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_texts=[len(doc) for doc in df2['cleaned review text']]\n",
    "df2['len_text']=len_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                         0\n",
      "App Version Code        1138\n",
      "App Version Name        1138\n",
      "Review Text                0\n",
      "Review Title               0\n",
      "Star Rating                0\n",
      "cleaned review text        0\n",
      "cleaned review title       0\n",
      "len title                 17\n",
      "len texts                 18\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df1['len title']=df1['len title'].replace(0,np.nan)\n",
    "df1['len texts']=df1['len texts'].replace(0,np.nan)\n",
    "print(np.sum(df1.isnull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                        0\n",
      "App Version Code        272\n",
      "App Version Name        272\n",
      "Review Text               0\n",
      "Review Title           1244\n",
      "cleaned review text       0\n",
      "len_texts                 0\n",
      "len_text                 11\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df2['len_text']=df2['len_text'].replace(0,np.nan)\n",
    "print(np.sum(df2.isnull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                         0\n",
      "App Version Code        1133\n",
      "App Version Name        1133\n",
      "Review Text                0\n",
      "Review Title               0\n",
      "Star Rating                0\n",
      "cleaned review text        0\n",
      "cleaned review title       0\n",
      "len title                  0\n",
      "len texts                  0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df1=df1.dropna(axis=0,subset=['len texts'])\n",
    "print(np.sum(df1.isnull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                        0\n",
      "App Version Code        269\n",
      "App Version Name        269\n",
      "Review Text               0\n",
      "Review Title           1234\n",
      "cleaned review text       0\n",
      "len_texts                 0\n",
      "len_text                  0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df2=df2.dropna(axis=0,subset=['len_text'])\n",
    "print(np.sum(df2.isnull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    2909\n",
       "1    1786\n",
       "4     610\n",
       "3     215\n",
       "2     154\n",
       "Name: Star Rating, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['Star Rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,test_x,train_y,test_y=train_test_split(df1['cleaned review text'],df1['Star Rating'],test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4539,)\n",
      "(1135,)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(test_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Countvectorizer with naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4456, 2000)\n",
      "(1114, 2000)\n",
      "(1114,)\n",
      "Test set prediction statistics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.80      0.88      0.84       347\n",
      "           2       0.25      0.03      0.05        36\n",
      "           3       0.00      0.00      0.00        40\n",
      "           4       0.26      0.05      0.08       125\n",
      "           5       0.77      0.95      0.85       566\n",
      "\n",
      "   micro avg       0.76      0.76      0.76      1114\n",
      "   macro avg       0.42      0.38      0.36      1114\n",
      "weighted avg       0.68      0.76      0.70      1114\n",
      "\n",
      "Train set prediction statistics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.85      0.89      0.87      1428\n",
      "           2       0.78      0.25      0.38       113\n",
      "           3       0.71      0.21      0.32       170\n",
      "           4       0.71      0.15      0.24       476\n",
      "           5       0.79      0.97      0.87      2269\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      4456\n",
      "   macro avg       0.77      0.49      0.54      4456\n",
      "weighted avg       0.80      0.81      0.77      4456\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec=CountVectorizer(max_features=2000)\n",
    "train_x1=vec.fit_transform(train_x)\n",
    "test_x1=vec.transform(test_x)\n",
    "print(train_x1.shape)\n",
    "print(test_x1.shape)\n",
    "clf=MultinomialNB()\n",
    "clf.fit(train_x1,train_y)\n",
    "predy_test=clf.predict(test_x1)\n",
    "print(predy_test.shape)\n",
    "print(\"Test set prediction statistics\")\n",
    "print(classification_report(test_y,predy_test))\n",
    "predy_train=clf.predict(train_x1)\n",
    "print(\"Train set prediction statistics\")\n",
    "print(classification_report(train_y,predy_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set prediction statistics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.79      0.92      0.85       347\n",
      "           2       0.00      0.00      0.00        36\n",
      "           3       0.00      0.00      0.00        40\n",
      "           4       0.40      0.02      0.03       125\n",
      "           5       0.77      0.95      0.85       566\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      1114\n",
      "   macro avg       0.39      0.38      0.35      1114\n",
      "weighted avg       0.68      0.77      0.70      1114\n",
      "\n",
      "Train set prediction statistics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.84      0.92      0.88      1428\n",
      "           2       0.94      0.15      0.26       113\n",
      "           3       0.87      0.15      0.26       170\n",
      "           4       0.87      0.12      0.22       476\n",
      "           5       0.79      0.97      0.87      2269\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      4456\n",
      "   macro avg       0.86      0.46      0.50      4456\n",
      "weighted avg       0.82      0.81      0.77      4456\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec1=CountVectorizer()\n",
    "train_x2=vec1.fit_transform(train_x)\n",
    "test_x2=vec1.transform(test_x)\n",
    "clf2=MultinomialNB()\n",
    "clf2.fit(train_x2,train_y)\n",
    "predy_test=clf2.predict(test_x2)\n",
    "predy_train=clf2.predict(train_x2)\n",
    "print(\"Test set prediction statistics\")\n",
    "print(classification_report(test_y,predy_test))\n",
    "print(\"Train set prediction statistics\")\n",
    "print(classification_report(train_y,predy_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tfidfVectorizer with MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set prediction statistics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.81      0.87      0.84       356\n",
      "           2       0.00      0.00      0.00        29\n",
      "           3       0.00      0.00      0.00        45\n",
      "           4       0.50      0.01      0.02       108\n",
      "           5       0.76      0.96      0.85       597\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      1135\n",
      "   macro avg       0.41      0.37      0.34      1135\n",
      "weighted avg       0.70      0.78      0.71      1135\n",
      "\n",
      "Train set prediction statistics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.85      0.92      0.88      1430\n",
      "           2       1.00      0.06      0.11       125\n",
      "           3       1.00      0.06      0.11       170\n",
      "           4       0.92      0.09      0.16       502\n",
      "           5       0.77      0.98      0.86      2312\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      4539\n",
      "   macro avg       0.91      0.42      0.43      4539\n",
      "weighted avg       0.83      0.80      0.74      4539\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vec3=TfidfVectorizer()\n",
    "train_x3=vec3.fit_transform(train_x)\n",
    "test_x3=vec3.transform(test_x)\n",
    "clf3=MultinomialNB()\n",
    "clf3.fit(train_x3,train_y)\n",
    "predy_test=clf2.predict(test_x3)\n",
    "predy_train=clf2.predict(train_x3)\n",
    "print(\"Test set prediction statistics\")\n",
    "print(classification_report(test_y,predy_test))\n",
    "print(\"Train set prediction statistics\")\n",
    "print(classification_report(train_y,predy_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set prediction statistics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.78      0.92      0.85       347\n",
      "           2       0.00      0.00      0.00        36\n",
      "           3       0.00      0.00      0.00        40\n",
      "           4       0.00      0.00      0.00       125\n",
      "           5       0.76      0.95      0.85       566\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      1114\n",
      "   macro avg       0.31      0.37      0.34      1114\n",
      "weighted avg       0.63      0.77      0.69      1114\n",
      "\n",
      "Train set prediction statistics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.82      0.93      0.87      1428\n",
      "           2       0.00      0.00      0.00       113\n",
      "           3       0.00      0.00      0.00       170\n",
      "           4       1.00      0.01      0.02       476\n",
      "           5       0.78      0.97      0.86      2269\n",
      "\n",
      "   micro avg       0.79      0.79      0.79      4456\n",
      "   macro avg       0.52      0.38      0.35      4456\n",
      "weighted avg       0.77      0.79      0.72      4456\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec4=TfidfVectorizer(max_features=3000)\n",
    "train_x4=vec4.fit_transform(train_x)\n",
    "test_x4=vec4.transform(test_x)\n",
    "clf4=MultinomialNB()\n",
    "clf4.fit(train_x4,train_y)\n",
    "predy_test=clf4.predict(test_x4)\n",
    "predy_train=clf4.predict(train_x4)\n",
    "print(\"Test set prediction statistics\")\n",
    "print(classification_report(test_y,predy_test))\n",
    "print(\"Train set prediction statistics\")\n",
    "print(classification_report(train_y,predy_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4408"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vec3.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tfidf with ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X_data, y_data, batch_size):\n",
    "    samples_per_epoch = X_data.shape[0]\n",
    "    number_of_batches = samples_per_epoch/batch_size\n",
    "    counter=0\n",
    "    index = np.arange(np.shape(y_data)[0])\n",
    "    while 1:\n",
    "        index_batch = index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X_data[index_batch,:].toarray()\n",
    "        #y_batch = y_data[y_data.index[index_batch]]\n",
    "        y_batch=y_data[index_batch,:]\n",
    "        counter += 1\n",
    "        yield X_batch,y_batch\n",
    "        if (counter > number_of_batches):\n",
    "            counter=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator2(X_data,batch_size):\n",
    "    samples_per_epoch = X_data.shape[0]\n",
    "    number_of_batches = samples_per_epoch/batch_size\n",
    "    counter=0\n",
    "    index = np.arange(X_data.shape[0])\n",
    "    while 1:\n",
    "        index_batch = index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X_data[index_batch,:].toarray()\n",
    "        #y_batch = y_data[y_data.index[index_batch]]\n",
    "        #y_batch=y_data[index_batch,:]\n",
    "        counter += 1\n",
    "        yield X_batch\n",
    "        if (counter > number_of_batches):\n",
    "            counter=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class Metrics(Callback):\n",
    "    def on_train_begin(self,logs={}):\n",
    "        self.val_f1s=[]\n",
    "    def on_epoch_end(self,epoch,logs={}):\n",
    "        pred_y=self.model.predict(self.validation_data[0])\n",
    "        pred_y=np.argmax(pred_y,axis=1)\n",
    "        val_tar=np.argmax(self.validation_data[1],axis=1)\n",
    "        score=f1_score(val_tar,pred_y,average='weighted')\n",
    "        self.val_f1s.append(score)\n",
    "        print(\"val weighted f1: \",score)\n",
    "        return\n",
    "metric1=Metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN with tfidfvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1=Sequential()\n",
    "model1.add(Dense(64,activation='relu',input_dim=len(vec3.vocabulary_)))\n",
    "model1.add(Dropout(0.2))\n",
    "model1.add(Dense(5,activation='softmax'))\n",
    "model1.compile(loss='categorical_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y1=train_y-1\n",
    "test_y1=test_y-1\n",
    "train_y_hotenc=to_categorical(train_y1)\n",
    "test_y_hotenc=to_categorical(test_y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "142/141 [==============================] - ETA: 2s - loss: 0.485 - ETA: 2s - loss: 0.404 - ETA: 2s - loss: 0.447 - ETA: 2s - loss: 0.423 - ETA: 2s - loss: 0.437 - ETA: 2s - loss: 0.444 - ETA: 2s - loss: 0.437 - ETA: 2s - loss: 0.436 - ETA: 2s - loss: 0.451 - ETA: 1s - loss: 0.465 - ETA: 1s - loss: 0.465 - ETA: 1s - loss: 0.468 - ETA: 1s - loss: 0.469 - ETA: 1s - loss: 0.475 - ETA: 1s - loss: 0.467 - ETA: 1s - loss: 0.469 - ETA: 1s - loss: 0.468 - ETA: 1s - loss: 0.472 - ETA: 1s - loss: 0.466 - ETA: 1s - loss: 0.465 - ETA: 1s - loss: 0.466 - ETA: 1s - loss: 0.470 - ETA: 0s - loss: 0.468 - ETA: 0s - loss: 0.466 - ETA: 0s - loss: 0.468 - ETA: 0s - loss: 0.469 - ETA: 0s - loss: 0.476 - ETA: 0s - loss: 0.481 - ETA: 0s - loss: 0.483 - ETA: 0s - loss: 0.485 - ETA: 0s - loss: 0.484 - ETA: 0s - loss: 0.484 - ETA: 0s - loss: 0.488 - ETA: 0s - loss: 0.485 - ETA: 0s - loss: 0.484 - ETA: 0s - loss: 0.484 - ETA: 0s - loss: 0.484 - ETA: 0s - loss: 0.485 - ETA: 0s - loss: 0.483 - ETA: 0s - loss: 0.482 - ETA: 0s - loss: 0.481 - 2s 17ms/step - loss: 0.4820 - val_loss: 0.6824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val weighted f1:  0.7284149773276202\n",
      "Epoch 2/5\n",
      "142/141 [==============================] - ETA: 1s - loss: 0.453 - ETA: 2s - loss: 0.377 - ETA: 2s - loss: 0.408 - ETA: 2s - loss: 0.383 - ETA: 2s - loss: 0.399 - ETA: 2s - loss: 0.406 - ETA: 2s - loss: 0.406 - ETA: 1s - loss: 0.402 - ETA: 1s - loss: 0.415 - ETA: 1s - loss: 0.426 - ETA: 1s - loss: 0.434 - ETA: 1s - loss: 0.424 - ETA: 1s - loss: 0.429 - ETA: 1s - loss: 0.431 - ETA: 1s - loss: 0.435 - ETA: 1s - loss: 0.433 - ETA: 1s - loss: 0.426 - ETA: 1s - loss: 0.431 - ETA: 1s - loss: 0.428 - ETA: 1s - loss: 0.428 - ETA: 1s - loss: 0.426 - ETA: 1s - loss: 0.422 - ETA: 1s - loss: 0.424 - ETA: 1s - loss: 0.424 - ETA: 1s - loss: 0.428 - ETA: 0s - loss: 0.426 - ETA: 0s - loss: 0.428 - ETA: 0s - loss: 0.429 - ETA: 0s - loss: 0.431 - ETA: 0s - loss: 0.439 - ETA: 0s - loss: 0.438 - ETA: 0s - loss: 0.440 - ETA: 0s - loss: 0.440 - ETA: 0s - loss: 0.440 - ETA: 0s - loss: 0.440 - ETA: 0s - loss: 0.443 - ETA: 0s - loss: 0.442 - ETA: 0s - loss: 0.441 - ETA: 0s - loss: 0.441 - ETA: 0s - loss: 0.441 - ETA: 0s - loss: 0.441 - ETA: 0s - loss: 0.440 - ETA: 0s - loss: 0.437 - ETA: 0s - loss: 0.437 - 3s 18ms/step - loss: 0.4373 - val_loss: 0.6981\n",
      "val weighted f1:  0.7286345847070841\n",
      "Epoch 3/5\n",
      "142/141 [==============================] - ETA: 1s - loss: 0.411 - ETA: 2s - loss: 0.337 - ETA: 2s - loss: 0.373 - ETA: 2s - loss: 0.362 - ETA: 2s - loss: 0.363 - ETA: 2s - loss: 0.370 - ETA: 2s - loss: 0.366 - ETA: 2s - loss: 0.362 - ETA: 2s - loss: 0.368 - ETA: 1s - loss: 0.379 - ETA: 1s - loss: 0.393 - ETA: 1s - loss: 0.389 - ETA: 1s - loss: 0.393 - ETA: 1s - loss: 0.395 - ETA: 1s - loss: 0.396 - ETA: 1s - loss: 0.395 - ETA: 1s - loss: 0.387 - ETA: 1s - loss: 0.390 - ETA: 1s - loss: 0.387 - ETA: 1s - loss: 0.387 - ETA: 1s - loss: 0.388 - ETA: 1s - loss: 0.382 - ETA: 1s - loss: 0.380 - ETA: 1s - loss: 0.384 - ETA: 1s - loss: 0.384 - ETA: 1s - loss: 0.387 - ETA: 1s - loss: 0.386 - ETA: 0s - loss: 0.388 - ETA: 0s - loss: 0.390 - ETA: 0s - loss: 0.396 - ETA: 0s - loss: 0.400 - ETA: 0s - loss: 0.400 - ETA: 0s - loss: 0.402 - ETA: 0s - loss: 0.403 - ETA: 0s - loss: 0.403 - ETA: 0s - loss: 0.404 - ETA: 0s - loss: 0.405 - ETA: 0s - loss: 0.404 - ETA: 0s - loss: 0.405 - ETA: 0s - loss: 0.404 - ETA: 0s - loss: 0.404 - ETA: 0s - loss: 0.404 - ETA: 0s - loss: 0.403 - ETA: 0s - loss: 0.401 - ETA: 0s - loss: 0.400 - 3s 19ms/step - loss: 0.4005 - val_loss: 0.7153\n",
      "val weighted f1:  0.7245497772741132\n",
      "Epoch 4/5\n",
      "142/141 [==============================] - ETA: 2s - loss: 0.383 - ETA: 2s - loss: 0.316 - ETA: 3s - loss: 0.334 - ETA: 2s - loss: 0.335 - ETA: 2s - loss: 0.338 - ETA: 2s - loss: 0.350 - ETA: 2s - loss: 0.350 - ETA: 2s - loss: 0.346 - ETA: 2s - loss: 0.342 - ETA: 2s - loss: 0.353 - ETA: 2s - loss: 0.363 - ETA: 2s - loss: 0.372 - ETA: 2s - loss: 0.366 - ETA: 2s - loss: 0.368 - ETA: 1s - loss: 0.370 - ETA: 1s - loss: 0.368 - ETA: 1s - loss: 0.369 - ETA: 1s - loss: 0.360 - ETA: 1s - loss: 0.364 - ETA: 1s - loss: 0.362 - ETA: 1s - loss: 0.362 - ETA: 1s - loss: 0.357 - ETA: 1s - loss: 0.354 - ETA: 1s - loss: 0.359 - ETA: 1s - loss: 0.360 - ETA: 1s - loss: 0.364 - ETA: 1s - loss: 0.362 - ETA: 1s - loss: 0.361 - ETA: 0s - loss: 0.364 - ETA: 0s - loss: 0.364 - ETA: 0s - loss: 0.368 - ETA: 0s - loss: 0.372 - ETA: 0s - loss: 0.372 - ETA: 0s - loss: 0.374 - ETA: 0s - loss: 0.374 - ETA: 0s - loss: 0.375 - ETA: 0s - loss: 0.374 - ETA: 0s - loss: 0.377 - ETA: 0s - loss: 0.376 - ETA: 0s - loss: 0.376 - ETA: 0s - loss: 0.376 - ETA: 0s - loss: 0.376 - ETA: 0s - loss: 0.375 - ETA: 0s - loss: 0.374 - ETA: 0s - loss: 0.372 - ETA: 0s - loss: 0.372 - 3s 20ms/step - loss: 0.3721 - val_loss: 0.7339\n",
      "val weighted f1:  0.7269067696665354\n",
      "Epoch 5/5\n",
      "142/141 [==============================] - ETA: 1s - loss: 0.369 - ETA: 2s - loss: 0.293 - ETA: 2s - loss: 0.307 - ETA: 2s - loss: 0.298 - ETA: 2s - loss: 0.300 - ETA: 2s - loss: 0.308 - ETA: 2s - loss: 0.308 - ETA: 2s - loss: 0.306 - ETA: 2s - loss: 0.305 - ETA: 2s - loss: 0.316 - ETA: 2s - loss: 0.328 - ETA: 2s - loss: 0.335 - ETA: 2s - loss: 0.334 - ETA: 2s - loss: 0.333 - ETA: 1s - loss: 0.335 - ETA: 1s - loss: 0.332 - ETA: 1s - loss: 0.334 - ETA: 1s - loss: 0.327 - ETA: 1s - loss: 0.328 - ETA: 1s - loss: 0.330 - ETA: 1s - loss: 0.327 - ETA: 1s - loss: 0.327 - ETA: 1s - loss: 0.325 - ETA: 1s - loss: 0.324 - ETA: 1s - loss: 0.326 - ETA: 1s - loss: 0.324 - ETA: 1s - loss: 0.328 - ETA: 1s - loss: 0.328 - ETA: 1s - loss: 0.326 - ETA: 1s - loss: 0.328 - ETA: 0s - loss: 0.329 - ETA: 0s - loss: 0.332 - ETA: 0s - loss: 0.337 - ETA: 0s - loss: 0.338 - ETA: 0s - loss: 0.340 - ETA: 0s - loss: 0.343 - ETA: 0s - loss: 0.343 - ETA: 0s - loss: 0.344 - ETA: 0s - loss: 0.345 - ETA: 0s - loss: 0.346 - ETA: 0s - loss: 0.345 - ETA: 0s - loss: 0.344 - ETA: 0s - loss: 0.344 - ETA: 0s - loss: 0.344 - ETA: 0s - loss: 0.344 - ETA: 0s - loss: 0.343 - ETA: 0s - loss: 0.341 - ETA: 0s - loss: 0.341 - 3s 20ms/step - loss: 0.3413 - val_loss: 0.7551\n",
      "val weighted f1:  0.7255546916218332\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15c2275ee80>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit_generator(generator=batch_generator(train_x3,train_y_hotenc, 32),\n",
    "                    epochs=5, validation_data=(test_x3, test_y_hotenc),steps_per_epoch=train_x3.shape[0]/32,callbacks=[metric1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN with countvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2=Sequential()\n",
    "model2.add(Dense(64,activation='relu',input_dim=len(vec1.vocabulary_)))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(5,activation='softmax'))\n",
    "model2.compile(loss='categorical_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "140/139 [==============================] - ETA: 0s - loss: 0.493 - ETA: 1s - loss: 0.358 - ETA: 1s - loss: 0.378 - ETA: 1s - loss: 0.378 - ETA: 0s - loss: 0.399 - ETA: 0s - loss: 0.399 - ETA: 0s - loss: 0.389 - ETA: 0s - loss: 0.403 - ETA: 0s - loss: 0.407 - ETA: 0s - loss: 0.407 - ETA: 0s - loss: 0.414 - ETA: 0s - loss: 0.416 - ETA: 0s - loss: 0.413 - ETA: 0s - loss: 0.409 - ETA: 0s - loss: 0.410 - ETA: 0s - loss: 0.408 - ETA: 0s - loss: 0.405 - 1s 7ms/step - loss: 0.4028 - val_loss: 0.7873\n",
      "val weighted f1:  0.6975569860340413\n",
      "Epoch 2/10\n",
      "140/139 [==============================] - ETA: 2s - loss: 0.386 - ETA: 1s - loss: 0.297 - ETA: 1s - loss: 0.332 - ETA: 0s - loss: 0.345 - ETA: 0s - loss: 0.348 - ETA: 0s - loss: 0.354 - ETA: 0s - loss: 0.352 - ETA: 0s - loss: 0.364 - ETA: 0s - loss: 0.364 - ETA: 0s - loss: 0.373 - ETA: 0s - loss: 0.376 - ETA: 0s - loss: 0.375 - ETA: 0s - loss: 0.372 - ETA: 0s - loss: 0.373 - ETA: 0s - loss: 0.371 - ETA: 0s - loss: 0.369 - 1s 7ms/step - loss: 0.3667 - val_loss: 0.8087\n",
      "val weighted f1:  0.697822398770991\n",
      "Epoch 3/10\n",
      "140/139 [==============================] - ETA: 2s - loss: 0.310 - ETA: 1s - loss: 0.269 - ETA: 1s - loss: 0.295 - ETA: 0s - loss: 0.326 - ETA: 0s - loss: 0.312 - ETA: 0s - loss: 0.322 - ETA: 0s - loss: 0.332 - ETA: 0s - loss: 0.337 - ETA: 0s - loss: 0.338 - ETA: 0s - loss: 0.347 - ETA: 0s - loss: 0.350 - ETA: 0s - loss: 0.346 - ETA: 0s - loss: 0.345 - ETA: 0s - loss: 0.344 - ETA: 0s - loss: 0.340 - 1s 7ms/step - loss: 0.3384 - val_loss: 0.8339\n",
      "val weighted f1:  0.7035994766230124\n",
      "Epoch 4/10\n",
      "140/139 [==============================] - ETA: 2s - loss: 0.291 - ETA: 1s - loss: 0.238 - ETA: 1s - loss: 0.265 - ETA: 0s - loss: 0.294 - ETA: 0s - loss: 0.285 - ETA: 0s - loss: 0.298 - ETA: 0s - loss: 0.311 - ETA: 0s - loss: 0.318 - ETA: 0s - loss: 0.318 - ETA: 0s - loss: 0.325 - ETA: 0s - loss: 0.329 - ETA: 0s - loss: 0.325 - ETA: 0s - loss: 0.322 - ETA: 0s - loss: 0.323 - ETA: 0s - loss: 0.321 - ETA: 0s - loss: 0.320 - 1s 7ms/step - loss: 0.3174 - val_loss: 0.8603\n",
      "val weighted f1:  0.699542414692878\n",
      "Epoch 5/10\n",
      "140/139 [==============================] - ETA: 2s - loss: 0.262 - ETA: 1s - loss: 0.230 - ETA: 1s - loss: 0.256 - ETA: 1s - loss: 0.255 - ETA: 0s - loss: 0.276 - ETA: 0s - loss: 0.281 - ETA: 0s - loss: 0.284 - ETA: 0s - loss: 0.294 - ETA: 0s - loss: 0.295 - ETA: 0s - loss: 0.299 - ETA: 0s - loss: 0.306 - ETA: 0s - loss: 0.308 - ETA: 0s - loss: 0.305 - ETA: 0s - loss: 0.303 - ETA: 0s - loss: 0.303 - ETA: 0s - loss: 0.301 - ETA: 0s - loss: 0.300 - 1s 8ms/step - loss: 0.2986 - val_loss: 0.8886\n",
      "val weighted f1:  0.6984946120150367\n",
      "Epoch 6/10\n",
      "140/139 [==============================] - ETA: 0s - loss: 0.295 - ETA: 1s - loss: 0.218 - ETA: 1s - loss: 0.232 - ETA: 0s - loss: 0.250 - ETA: 0s - loss: 0.252 - ETA: 0s - loss: 0.255 - ETA: 0s - loss: 0.256 - ETA: 0s - loss: 0.265 - ETA: 0s - loss: 0.267 - ETA: 0s - loss: 0.274 - ETA: 0s - loss: 0.276 - ETA: 0s - loss: 0.280 - ETA: 0s - loss: 0.287 - ETA: 0s - loss: 0.287 - ETA: 0s - loss: 0.285 - ETA: 0s - loss: 0.283 - ETA: 0s - loss: 0.284 - ETA: 0s - loss: 0.281 - ETA: 0s - loss: 0.283 - 1s 9ms/step - loss: 0.2817 - val_loss: 0.9155\n",
      "val weighted f1:  0.695427717455488\n",
      "Epoch 7/10\n",
      "140/139 [==============================] - ETA: 0s - loss: 0.271 - ETA: 0s - loss: 0.246 - ETA: 0s - loss: 0.232 - ETA: 0s - loss: 0.254 - ETA: 0s - loss: 0.248 - ETA: 0s - loss: 0.252 - ETA: 0s - loss: 0.266 - ETA: 0s - loss: 0.268 - ETA: 0s - loss: 0.269 - ETA: 0s - loss: 0.279 - ETA: 0s - loss: 0.282 - ETA: 0s - loss: 0.279 - ETA: 0s - loss: 0.277 - ETA: 0s - loss: 0.276 - ETA: 0s - loss: 0.275 - ETA: 0s - loss: 0.273 - ETA: 0s - loss: 0.274 - 1s 7ms/step - loss: 0.2729 - val_loss: 0.9403\n",
      "val weighted f1:  0.6959202155850837\n",
      "Epoch 8/10\n",
      "140/139 [==============================] - ETA: 2s - loss: 0.231 - ETA: 1s - loss: 0.183 - ETA: 0s - loss: 0.201 - ETA: 0s - loss: 0.226 - ETA: 0s - loss: 0.220 - ETA: 0s - loss: 0.232 - ETA: 0s - loss: 0.232 - ETA: 0s - loss: 0.247 - ETA: 0s - loss: 0.252 - ETA: 0s - loss: 0.254 - ETA: 0s - loss: 0.253 - ETA: 0s - loss: 0.260 - ETA: 0s - loss: 0.264 - ETA: 0s - loss: 0.261 - ETA: 0s - loss: 0.258 - ETA: 0s - loss: 0.260 - ETA: 0s - loss: 0.261 - ETA: 0s - loss: 0.261 - 1s 8ms/step - loss: 0.2590 - val_loss: 0.9676\n",
      "val weighted f1:  0.6939057173751733\n",
      "Epoch 9/10\n",
      "140/139 [==============================] - ETA: 2s - loss: 0.257 - ETA: 1s - loss: 0.196 - ETA: 1s - loss: 0.203 - ETA: 1s - loss: 0.211 - ETA: 1s - loss: 0.227 - ETA: 0s - loss: 0.217 - ETA: 0s - loss: 0.230 - ETA: 0s - loss: 0.242 - ETA: 0s - loss: 0.246 - ETA: 0s - loss: 0.247 - ETA: 0s - loss: 0.255 - ETA: 0s - loss: 0.257 - ETA: 0s - loss: 0.259 - ETA: 0s - loss: 0.257 - ETA: 0s - loss: 0.256 - ETA: 0s - loss: 0.256 - ETA: 0s - loss: 0.255 - ETA: 0s - loss: 0.254 - 1s 8ms/step - loss: 0.2523 - val_loss: 0.9943\n",
      "val weighted f1:  0.6923870522386092\n",
      "Epoch 10/10\n",
      "140/139 [==============================] - ETA: 0s - loss: 0.213 - ETA: 0s - loss: 0.214 - ETA: 0s - loss: 0.192 - ETA: 0s - loss: 0.218 - ETA: 0s - loss: 0.209 - ETA: 0s - loss: 0.223 - ETA: 0s - loss: 0.229 - ETA: 0s - loss: 0.239 - ETA: 0s - loss: 0.239 - ETA: 0s - loss: 0.240 - ETA: 0s - loss: 0.246 - ETA: 0s - loss: 0.245 - ETA: 0s - loss: 0.250 - ETA: 0s - loss: 0.248 - ETA: 0s - loss: 0.247 - ETA: 0s - loss: 0.246 - ETA: 0s - loss: 0.247 - ETA: 0s - loss: 0.247 - ETA: 0s - loss: 0.247 - 1s 8ms/step - loss: 0.2455 - val_loss: 1.0215\n",
      "val weighted f1:  0.6951644453847253\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15c29128f28>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit_generator(generator=batch_generator(train_x2,train_y_hotenc, 32),\n",
    "                    epochs=10, validation_data=(test_x2, test_y_hotenc),steps_per_epoch=train_x2.shape[0]/32,callbacks=[metric1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDFVectorizer with LinearSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set prediction statistics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.75      0.95      0.84       347\n",
      "           2       0.00      0.00      0.00        36\n",
      "           3       0.00      0.00      0.00        40\n",
      "           4       0.17      0.01      0.02       125\n",
      "           5       0.78      0.93      0.85       566\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      1114\n",
      "   macro avg       0.34      0.38      0.34      1114\n",
      "weighted avg       0.65      0.77      0.69      1114\n",
      "\n",
      "Train set prediction statistics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.81      0.97      0.88      1428\n",
      "           2       0.00      0.00      0.00       113\n",
      "           3       1.00      0.01      0.01       170\n",
      "           4       0.76      0.03      0.06       476\n",
      "           5       0.80      0.97      0.88      2269\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      4456\n",
      "   macro avg       0.67      0.39      0.37      4456\n",
      "weighted avg       0.79      0.81      0.74      4456\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf5=LinearSVC(C=0.1)\n",
    "clf5.fit(train_x4,train_y)\n",
    "predy_test=clf5.predict(test_x4)\n",
    "predy_train=clf5.predict(train_x4)\n",
    "print(\"Test set prediction statistics\")\n",
    "print(classification_report(test_y,predy_test))\n",
    "print(\"Train set prediction statistics\")\n",
    "print(classification_report(train_y,predy_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Word2vec as featurevector and passing it through different classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "docs=[doc.split() for doc in df1['cleaned review text']]\n",
    "model_w2v1=Word2Vec(docs,window=5,min_count=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index={t[0]:idx+1 for idx,t in enumerate(model_w2v1.wv.vocab.items())}\n",
    "inv_wrd_index={val:key for key,val in word_index.items()}\n",
    "vocab_len=len(model_w2v1.wv.vocab)\n",
    "vocab_len=vocab_len+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_mat=np.random.randn(vocab_len,100)\n",
    "for word,idx in word_index.items():\n",
    "    emb_mat[idx]=model_w2v1.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=[]\n",
    "for doc in df1['cleaned review text']:\n",
    "    text=[]\n",
    "    for word in doc.split():\n",
    "        if word in word_index.keys():\n",
    "            text.append(word_index[word])\n",
    "    docs.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                         0\n",
      "App Version Code        1133\n",
      "App Version Name        1133\n",
      "Review Text                0\n",
      "Review Title               0\n",
      "Star Rating                0\n",
      "cleaned review text        0\n",
      "cleaned review title       0\n",
      "len title                  0\n",
      "len texts                  0\n",
      "tokens cleaned texts       0\n",
      "len tokens               104\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "docs=np.array(docs)\n",
    "df1['tokens cleaned texts']=docs\n",
    "df1['len tokens']=[len(doc) for doc in df1['tokens cleaned texts']]\n",
    "df1['len tokens']=df1['len tokens'].replace(0,np.nan)\n",
    "print(np.sum(df1.isnull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                         0\n",
      "App Version Code        1099\n",
      "App Version Name        1099\n",
      "Review Text                0\n",
      "Review Title               0\n",
      "Star Rating                0\n",
      "cleaned review text        0\n",
      "cleaned review title       0\n",
      "len title                  0\n",
      "len texts                  0\n",
      "tokens cleaned texts       0\n",
      "len tokens                 0\n",
      "dtype: int64\n",
      "(5570, 12)\n"
     ]
    }
   ],
   "source": [
    "df1=df1.dropna(axis=0,subset=['len tokens'])\n",
    "print(np.sum(df1.isnull()))\n",
    "print(df1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "maxlen_texts=50\n",
    "docs=df1['tokens cleaned texts']\n",
    "docs2_X=pad_sequences(docs,padding='post',maxlen=maxlen_texts)\n",
    "word_index['UNK']=0\n",
    "inv_wrd_index[0]='UNK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedd_dim=100\n",
    "embedding_layer_texts=Embedding(vocab_len,embedd_dim,embeddings_initializer=Constant(emb_mat),\n",
    "                              input_length=maxlen_texts,trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 64)                276096    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 276,421\n",
      "Trainable params: 276,421\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv1D,MaxPooling1D,GlobalMaxPooling1D\n",
    "seq_input=Input(shape=(maxlen_texts,),dtype='int32')\n",
    "embed_mat=embedding_layer_texts(seq_input)\n",
    "x=Conv1D(filters=64,kernel_size=5,activation='relu')(embed_mat)\n",
    "x=MaxPooling1D(pool_size=5,strides=1)(x)\n",
    "x=GlobalMaxPooling1D()(x)\n",
    "preds=Dense(5,activation='softmax')(x)\n",
    "model3=Model(seq_input,preds)\n",
    "print(model3.summary())\n",
    "model3.compile(loss='sparse_categorical_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5570, 50)\n",
      "(5570,)\n"
     ]
    }
   ],
   "source": [
    "X=docs2_X\n",
    "Y=df1['Star Rating']-1\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5013 samples, validate on 557 samples\n",
      "Epoch 1/10\n",
      "5013/5013 [==============================] - ETA: 1:07 - loss: 2.265 - ETA: 18s - loss: 1.449 - ETA: 11s - loss: 1.35 - ETA: 8s - loss: 1.2665 - ETA: 7s - loss: 1.236 - ETA: 6s - loss: 1.249 - ETA: 5s - loss: 1.222 - ETA: 5s - loss: 1.202 - ETA: 4s - loss: 1.197 - ETA: 4s - loss: 1.197 - ETA: 4s - loss: 1.183 - ETA: 3s - loss: 1.180 - ETA: 3s - loss: 1.178 - ETA: 3s - loss: 1.168 - ETA: 3s - loss: 1.171 - ETA: 2s - loss: 1.152 - ETA: 2s - loss: 1.145 - ETA: 2s - loss: 1.144 - ETA: 2s - loss: 1.145 - ETA: 2s - loss: 1.134 - ETA: 2s - loss: 1.131 - ETA: 2s - loss: 1.125 - ETA: 2s - loss: 1.125 - ETA: 1s - loss: 1.121 - ETA: 1s - loss: 1.125 - ETA: 1s - loss: 1.128 - ETA: 1s - loss: 1.132 - ETA: 1s - loss: 1.136 - ETA: 1s - loss: 1.137 - ETA: 1s - loss: 1.131 - ETA: 1s - loss: 1.126 - ETA: 1s - loss: 1.126 - ETA: 1s - loss: 1.123 - ETA: 0s - loss: 1.123 - ETA: 0s - loss: 1.119 - ETA: 0s - loss: 1.118 - ETA: 0s - loss: 1.116 - ETA: 0s - loss: 1.117 - ETA: 0s - loss: 1.111 - ETA: 0s - loss: 1.106 - ETA: 0s - loss: 1.107 - ETA: 0s - loss: 1.110 - ETA: 0s - loss: 1.111 - ETA: 0s - loss: 1.109 - ETA: 0s - loss: 1.111 - ETA: 0s - loss: 1.112 - ETA: 0s - loss: 1.110 - 3s 647us/step - loss: 1.1116 - val_loss: 1.0343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val weighted f1:  0.5880861850443599\n",
      "Epoch 2/10\n",
      "5013/5013 [==============================] - ETA: 2s - loss: 1.183 - ETA: 2s - loss: 1.125 - ETA: 2s - loss: 1.087 - ETA: 2s - loss: 1.075 - ETA: 2s - loss: 1.041 - ETA: 2s - loss: 1.064 - ETA: 2s - loss: 1.052 - ETA: 2s - loss: 1.060 - ETA: 2s - loss: 1.066 - ETA: 2s - loss: 1.073 - ETA: 2s - loss: 1.061 - ETA: 2s - loss: 1.059 - ETA: 2s - loss: 1.077 - ETA: 1s - loss: 1.075 - ETA: 1s - loss: 1.073 - ETA: 1s - loss: 1.077 - ETA: 1s - loss: 1.088 - ETA: 1s - loss: 1.087 - ETA: 1s - loss: 1.090 - ETA: 1s - loss: 1.090 - ETA: 1s - loss: 1.095 - ETA: 1s - loss: 1.098 - ETA: 1s - loss: 1.096 - ETA: 1s - loss: 1.092 - ETA: 1s - loss: 1.092 - ETA: 1s - loss: 1.087 - ETA: 1s - loss: 1.080 - ETA: 1s - loss: 1.075 - ETA: 1s - loss: 1.079 - ETA: 0s - loss: 1.082 - ETA: 0s - loss: 1.084 - ETA: 0s - loss: 1.081 - ETA: 0s - loss: 1.086 - ETA: 0s - loss: 1.088 - ETA: 0s - loss: 1.088 - ETA: 0s - loss: 1.085 - ETA: 0s - loss: 1.084 - ETA: 0s - loss: 1.088 - ETA: 0s - loss: 1.085 - ETA: 0s - loss: 1.085 - ETA: 0s - loss: 1.085 - ETA: 0s - loss: 1.089 - ETA: 0s - loss: 1.089 - ETA: 0s - loss: 1.090 - ETA: 0s - loss: 1.086 - ETA: 0s - loss: 1.084 - ETA: 0s - loss: 1.082 - ETA: 0s - loss: 1.082 - 3s 549us/step - loss: 1.0815 - val_loss: 1.0199\n",
      "val weighted f1:  0.5826972010178118\n",
      "Epoch 3/10\n",
      "5013/5013 [==============================] - ETA: 2s - loss: 1.024 - ETA: 2s - loss: 1.088 - ETA: 2s - loss: 1.143 - ETA: 2s - loss: 1.138 - ETA: 2s - loss: 1.134 - ETA: 2s - loss: 1.115 - ETA: 2s - loss: 1.105 - ETA: 2s - loss: 1.102 - ETA: 2s - loss: 1.089 - ETA: 2s - loss: 1.102 - ETA: 2s - loss: 1.104 - ETA: 2s - loss: 1.107 - ETA: 2s - loss: 1.089 - ETA: 2s - loss: 1.083 - ETA: 2s - loss: 1.079 - ETA: 1s - loss: 1.079 - ETA: 1s - loss: 1.078 - ETA: 1s - loss: 1.074 - ETA: 1s - loss: 1.072 - ETA: 1s - loss: 1.072 - ETA: 1s - loss: 1.067 - ETA: 1s - loss: 1.067 - ETA: 1s - loss: 1.062 - ETA: 1s - loss: 1.067 - ETA: 1s - loss: 1.070 - ETA: 1s - loss: 1.070 - ETA: 1s - loss: 1.070 - ETA: 1s - loss: 1.068 - ETA: 1s - loss: 1.065 - ETA: 1s - loss: 1.064 - ETA: 1s - loss: 1.065 - ETA: 1s - loss: 1.066 - ETA: 0s - loss: 1.062 - ETA: 0s - loss: 1.058 - ETA: 0s - loss: 1.057 - ETA: 0s - loss: 1.056 - ETA: 0s - loss: 1.057 - ETA: 0s - loss: 1.058 - ETA: 0s - loss: 1.058 - ETA: 0s - loss: 1.056 - ETA: 0s - loss: 1.057 - ETA: 0s - loss: 1.061 - ETA: 0s - loss: 1.064 - ETA: 0s - loss: 1.062 - ETA: 0s - loss: 1.064 - ETA: 0s - loss: 1.066 - ETA: 0s - loss: 1.068 - ETA: 0s - loss: 1.070 - 3s 560us/step - loss: 1.0701 - val_loss: 1.0253\n",
      "val weighted f1:  0.5862944162436549\n",
      "Epoch 4/10\n",
      "5013/5013 [==============================] - ETA: 2s - loss: 0.981 - ETA: 2s - loss: 1.133 - ETA: 2s - loss: 1.061 - ETA: 2s - loss: 1.071 - ETA: 2s - loss: 1.057 - ETA: 2s - loss: 1.081 - ETA: 2s - loss: 1.077 - ETA: 2s - loss: 1.078 - ETA: 2s - loss: 1.063 - ETA: 2s - loss: 1.064 - ETA: 2s - loss: 1.073 - ETA: 2s - loss: 1.063 - ETA: 2s - loss: 1.055 - ETA: 2s - loss: 1.053 - ETA: 1s - loss: 1.059 - ETA: 1s - loss: 1.071 - ETA: 1s - loss: 1.072 - ETA: 1s - loss: 1.073 - ETA: 1s - loss: 1.076 - ETA: 1s - loss: 1.079 - ETA: 1s - loss: 1.072 - ETA: 1s - loss: 1.075 - ETA: 1s - loss: 1.075 - ETA: 1s - loss: 1.072 - ETA: 1s - loss: 1.077 - ETA: 1s - loss: 1.076 - ETA: 1s - loss: 1.073 - ETA: 1s - loss: 1.074 - ETA: 1s - loss: 1.069 - ETA: 1s - loss: 1.067 - ETA: 1s - loss: 1.064 - ETA: 0s - loss: 1.066 - ETA: 0s - loss: 1.070 - ETA: 0s - loss: 1.066 - ETA: 0s - loss: 1.067 - ETA: 0s - loss: 1.071 - ETA: 0s - loss: 1.073 - ETA: 0s - loss: 1.073 - ETA: 0s - loss: 1.077 - ETA: 0s - loss: 1.073 - ETA: 0s - loss: 1.075 - ETA: 0s - loss: 1.074 - ETA: 0s - loss: 1.072 - ETA: 0s - loss: 1.071 - ETA: 0s - loss: 1.069 - ETA: 0s - loss: 1.070 - ETA: 0s - loss: 1.070 - ETA: 0s - loss: 1.070 - ETA: 0s - loss: 1.072 - 3s 553us/step - loss: 1.0729 - val_loss: 1.0384\n",
      "val weighted f1:  0.5513654096228868\n",
      "Epoch 5/10\n",
      "5013/5013 [==============================] - ETA: 2s - loss: 0.967 - ETA: 2s - loss: 0.979 - ETA: 2s - loss: 0.934 - ETA: 2s - loss: 0.982 - ETA: 2s - loss: 1.000 - ETA: 2s - loss: 1.004 - ETA: 2s - loss: 0.996 - ETA: 2s - loss: 1.028 - ETA: 2s - loss: 1.029 - ETA: 2s - loss: 1.036 - ETA: 2s - loss: 1.046 - ETA: 2s - loss: 1.045 - ETA: 2s - loss: 1.045 - ETA: 2s - loss: 1.040 - ETA: 2s - loss: 1.045 - ETA: 1s - loss: 1.059 - ETA: 1s - loss: 1.060 - ETA: 1s - loss: 1.069 - ETA: 1s - loss: 1.071 - ETA: 1s - loss: 1.073 - ETA: 1s - loss: 1.076 - ETA: 1s - loss: 1.076 - ETA: 1s - loss: 1.079 - ETA: 1s - loss: 1.079 - ETA: 1s - loss: 1.078 - ETA: 1s - loss: 1.071 - ETA: 1s - loss: 1.073 - ETA: 1s - loss: 1.069 - ETA: 1s - loss: 1.069 - ETA: 1s - loss: 1.066 - ETA: 1s - loss: 1.062 - ETA: 1s - loss: 1.060 - ETA: 0s - loss: 1.059 - ETA: 0s - loss: 1.059 - ETA: 0s - loss: 1.058 - ETA: 0s - loss: 1.062 - ETA: 0s - loss: 1.058 - ETA: 0s - loss: 1.056 - ETA: 0s - loss: 1.060 - ETA: 0s - loss: 1.062 - ETA: 0s - loss: 1.066 - ETA: 0s - loss: 1.070 - ETA: 0s - loss: 1.065 - ETA: 0s - loss: 1.069 - ETA: 0s - loss: 1.067 - ETA: 0s - loss: 1.064 - ETA: 0s - loss: 1.066 - ETA: 0s - loss: 1.067 - ETA: 0s - loss: 1.069 - ETA: 0s - loss: 1.071 - ETA: 0s - loss: 1.073 - 3s 572us/step - loss: 1.0712 - val_loss: 1.0047\n",
      "val weighted f1:  0.45706371191135736\n",
      "Epoch 6/10\n",
      "5013/5013 [==============================] - ETA: 2s - loss: 0.840 - ETA: 2s - loss: 0.925 - ETA: 2s - loss: 1.028 - ETA: 2s - loss: 1.050 - ETA: 2s - loss: 1.068 - ETA: 2s - loss: 1.062 - ETA: 2s - loss: 1.045 - ETA: 2s - loss: 1.054 - ETA: 2s - loss: 1.055 - ETA: 2s - loss: 1.072 - ETA: 2s - loss: 1.061 - ETA: 2s - loss: 1.058 - ETA: 2s - loss: 1.046 - ETA: 1s - loss: 1.050 - ETA: 1s - loss: 1.060 - ETA: 1s - loss: 1.055 - ETA: 1s - loss: 1.061 - ETA: 1s - loss: 1.051 - ETA: 1s - loss: 1.047 - ETA: 1s - loss: 1.048 - ETA: 1s - loss: 1.048 - ETA: 1s - loss: 1.053 - ETA: 1s - loss: 1.053 - ETA: 1s - loss: 1.052 - ETA: 1s - loss: 1.051 - ETA: 1s - loss: 1.049 - ETA: 1s - loss: 1.052 - ETA: 1s - loss: 1.050 - ETA: 1s - loss: 1.046 - ETA: 1s - loss: 1.045 - ETA: 1s - loss: 1.048 - ETA: 1s - loss: 1.048 - ETA: 0s - loss: 1.048 - ETA: 0s - loss: 1.053 - ETA: 0s - loss: 1.053 - ETA: 0s - loss: 1.056 - ETA: 0s - loss: 1.055 - ETA: 0s - loss: 1.060 - ETA: 0s - loss: 1.065 - ETA: 0s - loss: 1.065 - ETA: 0s - loss: 1.065 - ETA: 0s - loss: 1.063 - ETA: 0s - loss: 1.060 - ETA: 0s - loss: 1.059 - ETA: 0s - loss: 1.056 - ETA: 0s - loss: 1.056 - ETA: 0s - loss: 1.063 - ETA: 0s - loss: 1.063 - ETA: 0s - loss: 1.064 - ETA: 0s - loss: 1.063 - 3s 576us/step - loss: 1.0617 - val_loss: 1.0054\n",
      "val weighted f1:  0.5026881720430108\n",
      "Epoch 7/10\n",
      "5013/5013 [==============================] - ETA: 2s - loss: 1.134 - ETA: 2s - loss: 1.088 - ETA: 2s - loss: 1.077 - ETA: 2s - loss: 1.066 - ETA: 2s - loss: 1.070 - ETA: 2s - loss: 1.069 - ETA: 2s - loss: 1.092 - ETA: 2s - loss: 1.101 - ETA: 2s - loss: 1.082 - ETA: 2s - loss: 1.083 - ETA: 2s - loss: 1.083 - ETA: 2s - loss: 1.074 - ETA: 2s - loss: 1.074 - ETA: 2s - loss: 1.068 - ETA: 2s - loss: 1.063 - ETA: 1s - loss: 1.043 - ETA: 1s - loss: 1.057 - ETA: 1s - loss: 1.056 - ETA: 1s - loss: 1.053 - ETA: 1s - loss: 1.053 - ETA: 1s - loss: 1.047 - ETA: 1s - loss: 1.044 - ETA: 1s - loss: 1.051 - ETA: 1s - loss: 1.052 - ETA: 1s - loss: 1.055 - ETA: 1s - loss: 1.055 - ETA: 1s - loss: 1.055 - ETA: 1s - loss: 1.055 - ETA: 1s - loss: 1.052 - ETA: 1s - loss: 1.051 - ETA: 1s - loss: 1.049 - ETA: 0s - loss: 1.049 - ETA: 0s - loss: 1.051 - ETA: 0s - loss: 1.051 - ETA: 0s - loss: 1.055 - ETA: 0s - loss: 1.059 - ETA: 0s - loss: 1.059 - ETA: 0s - loss: 1.060 - ETA: 0s - loss: 1.060 - ETA: 0s - loss: 1.061 - ETA: 0s - loss: 1.057 - ETA: 0s - loss: 1.058 - ETA: 0s - loss: 1.056 - ETA: 0s - loss: 1.057 - ETA: 0s - loss: 1.055 - ETA: 0s - loss: 1.057 - ETA: 0s - loss: 1.057 - ETA: 0s - loss: 1.058 - ETA: 0s - loss: 1.062 - ETA: 0s - loss: 1.063 - 3s 580us/step - loss: 1.0634 - val_loss: 1.0283\n",
      "val weighted f1:  0.2808641975308642\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5013/5013 [==============================] - ETA: 2s - loss: 1.161 - ETA: 2s - loss: 1.047 - ETA: 2s - loss: 1.035 - ETA: 2s - loss: 1.060 - ETA: 2s - loss: 1.074 - ETA: 2s - loss: 1.062 - ETA: 2s - loss: 1.069 - ETA: 2s - loss: 1.080 - ETA: 2s - loss: 1.075 - ETA: 2s - loss: 1.060 - ETA: 2s - loss: 1.056 - ETA: 2s - loss: 1.060 - ETA: 2s - loss: 1.061 - ETA: 2s - loss: 1.073 - ETA: 1s - loss: 1.072 - ETA: 1s - loss: 1.075 - ETA: 1s - loss: 1.076 - ETA: 1s - loss: 1.074 - ETA: 1s - loss: 1.078 - ETA: 1s - loss: 1.071 - ETA: 1s - loss: 1.072 - ETA: 1s - loss: 1.066 - ETA: 1s - loss: 1.067 - ETA: 1s - loss: 1.068 - ETA: 1s - loss: 1.065 - ETA: 1s - loss: 1.063 - ETA: 1s - loss: 1.068 - ETA: 1s - loss: 1.062 - ETA: 1s - loss: 1.058 - ETA: 0s - loss: 1.054 - ETA: 0s - loss: 1.051 - ETA: 0s - loss: 1.054 - ETA: 0s - loss: 1.050 - ETA: 0s - loss: 1.051 - ETA: 0s - loss: 1.050 - ETA: 0s - loss: 1.050 - ETA: 0s - loss: 1.053 - ETA: 0s - loss: 1.053 - ETA: 0s - loss: 1.057 - ETA: 0s - loss: 1.056 - ETA: 0s - loss: 1.056 - ETA: 0s - loss: 1.058 - ETA: 0s - loss: 1.057 - ETA: 0s - loss: 1.056 - ETA: 0s - loss: 1.054 - ETA: 0s - loss: 1.057 - ETA: 0s - loss: 1.059 - ETA: 0s - loss: 1.059 - 3s 554us/step - loss: 1.0585 - val_loss: 1.0160\n",
      "val weighted f1:  0.47187928669410156\n",
      "Epoch 9/10\n",
      "5013/5013 [==============================] - ETA: 3s - loss: 1.080 - ETA: 2s - loss: 1.037 - ETA: 2s - loss: 1.029 - ETA: 2s - loss: 1.064 - ETA: 2s - loss: 1.047 - ETA: 2s - loss: 1.034 - ETA: 2s - loss: 1.033 - ETA: 2s - loss: 1.013 - ETA: 2s - loss: 1.010 - ETA: 2s - loss: 1.018 - ETA: 2s - loss: 1.033 - ETA: 2s - loss: 1.034 - ETA: 2s - loss: 1.032 - ETA: 2s - loss: 1.033 - ETA: 2s - loss: 1.040 - ETA: 1s - loss: 1.035 - ETA: 1s - loss: 1.037 - ETA: 1s - loss: 1.035 - ETA: 1s - loss: 1.041 - ETA: 1s - loss: 1.047 - ETA: 1s - loss: 1.053 - ETA: 1s - loss: 1.055 - ETA: 1s - loss: 1.056 - ETA: 1s - loss: 1.048 - ETA: 1s - loss: 1.046 - ETA: 1s - loss: 1.039 - ETA: 1s - loss: 1.036 - ETA: 1s - loss: 1.044 - ETA: 1s - loss: 1.042 - ETA: 1s - loss: 1.044 - ETA: 1s - loss: 1.045 - ETA: 1s - loss: 1.042 - ETA: 1s - loss: 1.041 - ETA: 1s - loss: 1.043 - ETA: 0s - loss: 1.041 - ETA: 0s - loss: 1.040 - ETA: 0s - loss: 1.038 - ETA: 0s - loss: 1.040 - ETA: 0s - loss: 1.043 - ETA: 0s - loss: 1.044 - ETA: 0s - loss: 1.044 - ETA: 0s - loss: 1.043 - ETA: 0s - loss: 1.046 - ETA: 0s - loss: 1.047 - ETA: 0s - loss: 1.045 - ETA: 0s - loss: 1.044 - ETA: 0s - loss: 1.048 - ETA: 0s - loss: 1.048 - ETA: 0s - loss: 1.051 - ETA: 0s - loss: 1.051 - ETA: 0s - loss: 1.050 - ETA: 0s - loss: 1.051 - 3s 583us/step - loss: 1.0516 - val_loss: 1.0160\n",
      "val weighted f1:  0.5513654096228868\n",
      "Epoch 10/10\n",
      "5013/5013 [==============================] - ETA: 2s - loss: 0.890 - ETA: 2s - loss: 1.149 - ETA: 2s - loss: 1.171 - ETA: 2s - loss: 1.105 - ETA: 2s - loss: 1.097 - ETA: 2s - loss: 1.101 - ETA: 2s - loss: 1.100 - ETA: 2s - loss: 1.105 - ETA: 2s - loss: 1.099 - ETA: 2s - loss: 1.097 - ETA: 2s - loss: 1.087 - ETA: 2s - loss: 1.083 - ETA: 2s - loss: 1.084 - ETA: 2s - loss: 1.100 - ETA: 2s - loss: 1.096 - ETA: 2s - loss: 1.099 - ETA: 2s - loss: 1.101 - ETA: 2s - loss: 1.092 - ETA: 1s - loss: 1.085 - ETA: 1s - loss: 1.081 - ETA: 1s - loss: 1.087 - ETA: 1s - loss: 1.080 - ETA: 1s - loss: 1.081 - ETA: 1s - loss: 1.074 - ETA: 1s - loss: 1.072 - ETA: 1s - loss: 1.075 - ETA: 1s - loss: 1.080 - ETA: 1s - loss: 1.077 - ETA: 1s - loss: 1.074 - ETA: 1s - loss: 1.068 - ETA: 1s - loss: 1.067 - ETA: 1s - loss: 1.068 - ETA: 0s - loss: 1.060 - ETA: 0s - loss: 1.059 - ETA: 0s - loss: 1.055 - ETA: 0s - loss: 1.051 - ETA: 0s - loss: 1.056 - ETA: 0s - loss: 1.056 - ETA: 0s - loss: 1.057 - ETA: 0s - loss: 1.054 - ETA: 0s - loss: 1.052 - ETA: 0s - loss: 1.053 - ETA: 0s - loss: 1.053 - ETA: 0s - loss: 1.056 - ETA: 0s - loss: 1.054 - ETA: 0s - loss: 1.054 - ETA: 0s - loss: 1.053 - 3s 557us/step - loss: 1.0528 - val_loss: 1.0039\n",
      "val weighted f1:  0.5494791666666666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15c23ea9e80>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_x,test_x,train_y,test_y=train_test_split(X,Y,test_size=0.1)\n",
    "model3.fit(train_x,train_y,validation_data=(test_x,test_y),epochs=10,callbacks=[metric1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=df1['tokens cleaned texts']\n",
    "docs_X=np.zeros((docs.shape[0],embedd_dim))\n",
    "for idx,doc in enumerate(docs):\n",
    "    X=np.zeros((1,embedd_dim))\n",
    "    for word in doc:\n",
    "        v=model_w2v1.wv[inv_wrd_index[word]].reshape((1,embedd_dim))\n",
    "        X+=v\n",
    "    X/=len(doc)\n",
    "    docs_X[idx]=X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4=Sequential()\n",
    "model4.add(Dense(64,activation='relu',input_dim=embedd_dim))\n",
    "#model4.add(Dropout(0.2))\n",
    "model4.add(Dense(5,activation='softmax'))\n",
    "model4.compile(loss='sparse_categorical_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5013 samples, validate on 557 samples\n",
      "Epoch 1/20\n",
      "5013/5013 [==============================] - ETA: 55s - loss: 1.61 - ETA: 2s - loss: 1.3717 - ETA: 1s - loss: 1.275 - ETA: 0s - loss: 1.246 - ETA: 0s - loss: 1.238 - ETA: 0s - loss: 1.232 - ETA: 0s - loss: 1.226 - ETA: 0s - loss: 1.230 - ETA: 0s - loss: 1.223 - ETA: 0s - loss: 1.217 - ETA: 0s - loss: 1.212 - 1s 195us/step - loss: 1.2113 - val_loss: 1.1834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val weighted f1:  0.0\n",
      "Epoch 2/20\n",
      "5013/5013 [==============================] - ETA: 0s - loss: 1.070 - ETA: 0s - loss: 1.176 - ETA: 0s - loss: 1.185 - ETA: 0s - loss: 1.181 - ETA: 0s - loss: 1.165 - ETA: 0s - loss: 1.188 - ETA: 0s - loss: 1.177 - ETA: 0s - loss: 1.182 - ETA: 0s - loss: 1.182 - ETA: 0s - loss: 1.182 - ETA: 0s - loss: 1.181 - ETA: 0s - loss: 1.182 - 1s 124us/step - loss: 1.1796 - val_loss: 1.1687\n",
      "val weighted f1:  0.0\n",
      "Epoch 3/20\n",
      "5013/5013 [==============================] - ETA: 0s - loss: 1.013 - ETA: 0s - loss: 1.126 - ETA: 0s - loss: 1.182 - ETA: 0s - loss: 1.176 - ETA: 0s - loss: 1.183 - ETA: 0s - loss: 1.184 - ETA: 0s - loss: 1.182 - ETA: 0s - loss: 1.181 - ETA: 0s - loss: 1.178 - ETA: 0s - loss: 1.168 - ETA: 0s - loss: 1.164 - ETA: 0s - loss: 1.166 - 1s 124us/step - loss: 1.1651 - val_loss: 1.1672\n",
      "val weighted f1:  0.017793594306049824\n",
      "Epoch 4/20\n",
      "5013/5013 [==============================] - ETA: 0s - loss: 1.091 - ETA: 0s - loss: 1.145 - ETA: 0s - loss: 1.155 - ETA: 0s - loss: 1.160 - ETA: 0s - loss: 1.151 - ETA: 0s - loss: 1.150 - ETA: 0s - loss: 1.148 - ETA: 0s - loss: 1.149 - ETA: 0s - loss: 1.148 - ETA: 0s - loss: 1.148 - ETA: 0s - loss: 1.152 - ETA: 0s - loss: 1.151 - ETA: 0s - loss: 1.152 - 1s 131us/step - loss: 1.1522 - val_loss: 1.1511\n",
      "val weighted f1:  0.024822695035460998\n",
      "Epoch 5/20\n",
      "5013/5013 [==============================] - ETA: 0s - loss: 1.200 - ETA: 0s - loss: 1.155 - ETA: 0s - loss: 1.157 - ETA: 0s - loss: 1.153 - ETA: 0s - loss: 1.137 - ETA: 0s - loss: 1.145 - ETA: 0s - loss: 1.142 - ETA: 0s - loss: 1.145 - ETA: 0s - loss: 1.142 - ETA: 0s - loss: 1.142 - ETA: 0s - loss: 1.142 - ETA: 0s - loss: 1.143 - 1s 128us/step - loss: 1.1397 - val_loss: 1.1417\n",
      "val weighted f1:  0.045614035087719294\n",
      "Epoch 6/20\n",
      "5013/5013 [==============================] - ETA: 0s - loss: 1.212 - ETA: 0s - loss: 1.108 - ETA: 0s - loss: 1.106 - ETA: 0s - loss: 1.113 - ETA: 0s - loss: 1.120 - ETA: 0s - loss: 1.145 - ETA: 0s - loss: 1.156 - ETA: 0s - loss: 1.149 - ETA: 0s - loss: 1.142 - ETA: 0s - loss: 1.132 - ETA: 0s - loss: 1.137 - ETA: 0s - loss: 1.133 - ETA: 0s - loss: 1.127 - 1s 140us/step - loss: 1.1337 - val_loss: 1.1306\n",
      "val weighted f1:  0.08919382504288163\n",
      "Epoch 7/20\n",
      "5013/5013 [==============================] - ETA: 1s - loss: 1.133 - ETA: 0s - loss: 1.090 - ETA: 0s - loss: 1.138 - ETA: 0s - loss: 1.142 - ETA: 0s - loss: 1.128 - ETA: 0s - loss: 1.122 - ETA: 0s - loss: 1.109 - ETA: 0s - loss: 1.104 - ETA: 0s - loss: 1.106 - ETA: 0s - loss: 1.108 - ETA: 0s - loss: 1.109 - ETA: 0s - loss: 1.111 - ETA: 0s - loss: 1.111 - ETA: 0s - loss: 1.114 - 1s 150us/step - loss: 1.1141 - val_loss: 1.1210\n",
      "val weighted f1:  0.16171617161716173\n",
      "Epoch 8/20\n",
      "5013/5013 [==============================] - ETA: 0s - loss: 1.289 - ETA: 0s - loss: 1.103 - ETA: 0s - loss: 1.131 - ETA: 0s - loss: 1.144 - ETA: 0s - loss: 1.126 - ETA: 0s - loss: 1.123 - ETA: 0s - loss: 1.120 - ETA: 0s - loss: 1.106 - ETA: 0s - loss: 1.109 - ETA: 0s - loss: 1.112 - 1s 104us/step - loss: 1.1092 - val_loss: 1.1108\n",
      "val weighted f1:  0.19155844155844157\n",
      "Epoch 9/20\n",
      "5013/5013 [==============================] - ETA: 0s - loss: 1.044 - ETA: 0s - loss: 1.103 - ETA: 0s - loss: 1.089 - ETA: 0s - loss: 1.092 - ETA: 0s - loss: 1.084 - ETA: 0s - loss: 1.088 - ETA: 0s - loss: 1.097 - ETA: 0s - loss: 1.093 - ETA: 0s - loss: 1.092 - ETA: 0s - loss: 1.092 - ETA: 0s - loss: 1.092 - ETA: 0s - loss: 1.097 - 1s 127us/step - loss: 1.0947 - val_loss: 1.1069\n",
      "val weighted f1:  0.12773109243697478\n",
      "Epoch 10/20\n",
      "5013/5013 [==============================] - ETA: 0s - loss: 1.041 - ETA: 0s - loss: 1.082 - ETA: 0s - loss: 1.064 - ETA: 0s - loss: 1.078 - ETA: 0s - loss: 1.084 - ETA: 0s - loss: 1.081 - ETA: 0s - loss: 1.079 - ETA: 0s - loss: 1.073 - ETA: 0s - loss: 1.076 - ETA: 0s - loss: 1.075 - ETA: 0s - loss: 1.083 - ETA: 0s - loss: 1.079 - 1s 123us/step - loss: 1.0794 - val_loss: 1.0962\n",
      "val weighted f1:  0.2620904836193448\n",
      "Epoch 11/20\n",
      "5013/5013 [==============================] - ETA: 0s - loss: 0.912 - ETA: 0s - loss: 1.047 - ETA: 0s - loss: 1.022 - ETA: 0s - loss: 1.044 - ETA: 0s - loss: 1.048 - ETA: 0s - loss: 1.059 - ETA: 0s - loss: 1.073 - ETA: 0s - loss: 1.073 - ETA: 0s - loss: 1.077 - ETA: 0s - loss: 1.074 - ETA: 0s - loss: 1.071 - ETA: 0s - loss: 1.068 - ETA: 0s - loss: 1.069 - 1s 135us/step - loss: 1.0705 - val_loss: 1.1091\n",
      "val weighted f1:  0.5569948186528497\n",
      "Epoch 12/20\n",
      "5013/5013 [==============================] - ETA: 0s - loss: 1.327 - ETA: 0s - loss: 1.091 - ETA: 0s - loss: 1.075 - ETA: 0s - loss: 1.081 - ETA: 0s - loss: 1.077 - ETA: 0s - loss: 1.070 - ETA: 0s - loss: 1.065 - ETA: 0s - loss: 1.058 - ETA: 0s - loss: 1.053 - ETA: 0s - loss: 1.061 - ETA: 0s - loss: 1.057 - ETA: 0s - loss: 1.057 - ETA: 0s - loss: 1.060 - ETA: 0s - loss: 1.060 - 1s 146us/step - loss: 1.0612 - val_loss: 1.0781\n",
      "val weighted f1:  0.31212121212121213\n",
      "Epoch 13/20\n",
      "5013/5013 [==============================] - ETA: 0s - loss: 1.032 - ETA: 0s - loss: 1.058 - ETA: 0s - loss: 1.022 - ETA: 0s - loss: 1.003 - ETA: 0s - loss: 1.043 - ETA: 0s - loss: 1.047 - ETA: 0s - loss: 1.054 - ETA: 0s - loss: 1.061 - ETA: 0s - loss: 1.056 - ETA: 0s - loss: 1.056 - ETA: 0s - loss: 1.056 - ETA: 0s - loss: 1.060 - ETA: 0s - loss: 1.055 - 1s 134us/step - loss: 1.0549 - val_loss: 1.0845\n",
      "val weighted f1:  0.5361366622864652\n",
      "Epoch 14/20\n",
      "5013/5013 [==============================] - ETA: 0s - loss: 1.101 - ETA: 0s - loss: 1.078 - ETA: 0s - loss: 1.048 - ETA: 0s - loss: 1.060 - ETA: 0s - loss: 1.072 - ETA: 0s - loss: 1.070 - ETA: 0s - loss: 1.059 - ETA: 0s - loss: 1.054 - ETA: 0s - loss: 1.043 - ETA: 0s - loss: 1.043 - ETA: 0s - loss: 1.039 - ETA: 0s - loss: 1.042 - ETA: 0s - loss: 1.046 - 1s 140us/step - loss: 1.0466 - val_loss: 1.0633\n",
      "val weighted f1:  0.3593519882179676\n",
      "Epoch 15/20\n",
      "5013/5013 [==============================] - ETA: 1s - loss: 0.994 - ETA: 0s - loss: 1.059 - ETA: 0s - loss: 1.053 - ETA: 0s - loss: 1.046 - ETA: 0s - loss: 1.059 - ETA: 0s - loss: 1.057 - ETA: 0s - loss: 1.044 - ETA: 0s - loss: 1.033 - ETA: 0s - loss: 1.022 - ETA: 0s - loss: 1.024 - ETA: 0s - loss: 1.028 - ETA: 0s - loss: 1.027 - ETA: 0s - loss: 1.028 - ETA: 0s - loss: 1.033 - 1s 147us/step - loss: 1.0352 - val_loss: 1.0682\n",
      "val weighted f1:  0.4822888283378746\n",
      "Epoch 16/20\n",
      "5013/5013 [==============================] - ETA: 0s - loss: 0.891 - ETA: 0s - loss: 1.023 - ETA: 0s - loss: 1.017 - ETA: 0s - loss: 1.019 - ETA: 0s - loss: 1.017 - ETA: 0s - loss: 1.016 - ETA: 0s - loss: 1.025 - ETA: 0s - loss: 1.029 - ETA: 0s - loss: 1.026 - ETA: 0s - loss: 1.032 - ETA: 0s - loss: 1.034 - ETA: 0s - loss: 1.031 - ETA: 0s - loss: 1.032 - 1s 141us/step - loss: 1.0345 - val_loss: 1.0545\n",
      "val weighted f1:  0.4108416547788873\n",
      "Epoch 17/20\n",
      "5013/5013 [==============================] - ETA: 0s - loss: 0.998 - ETA: 0s - loss: 1.048 - ETA: 0s - loss: 1.023 - ETA: 0s - loss: 1.017 - ETA: 0s - loss: 1.031 - ETA: 0s - loss: 1.030 - ETA: 0s - loss: 1.022 - ETA: 0s - loss: 1.026 - ETA: 0s - loss: 1.023 - ETA: 0s - loss: 1.023 - ETA: 0s - loss: 1.023 - ETA: 0s - loss: 1.023 - ETA: 0s - loss: 1.023 - ETA: 0s - loss: 1.022 - 1s 149us/step - loss: 1.0252 - val_loss: 1.0598\n",
      "val weighted f1:  0.2782071097372488\n",
      "Epoch 18/20\n",
      "5013/5013 [==============================] - ETA: 0s - loss: 1.060 - ETA: 0s - loss: 1.067 - ETA: 0s - loss: 1.046 - ETA: 0s - loss: 1.014 - ETA: 0s - loss: 1.014 - ETA: 0s - loss: 1.007 - ETA: 0s - loss: 1.014 - ETA: 0s - loss: 1.026 - ETA: 0s - loss: 1.023 - ETA: 0s - loss: 1.022 - ETA: 0s - loss: 1.026 - ETA: 0s - loss: 1.025 - ETA: 0s - loss: 1.029 - ETA: 0s - loss: 1.025 - 1s 144us/step - loss: 1.0236 - val_loss: 1.0457\n",
      "val weighted f1:  0.40172166427546624\n",
      "Epoch 19/20\n",
      "5013/5013 [==============================] - ETA: 0s - loss: 1.298 - ETA: 0s - loss: 1.037 - ETA: 0s - loss: 1.018 - ETA: 0s - loss: 1.007 - ETA: 0s - loss: 1.010 - ETA: 0s - loss: 1.013 - ETA: 0s - loss: 1.014 - ETA: 0s - loss: 0.997 - ETA: 0s - loss: 0.997 - ETA: 0s - loss: 1.003 - ETA: 0s - loss: 1.007 - ETA: 0s - loss: 1.009 - ETA: 0s - loss: 1.010 - ETA: 0s - loss: 1.018 - 1s 144us/step - loss: 1.0202 - val_loss: 1.0461\n",
      "val weighted f1:  0.3545051698670606\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5013/5013 [==============================] - ETA: 0s - loss: 1.141 - ETA: 0s - loss: 0.956 - ETA: 0s - loss: 0.983 - ETA: 0s - loss: 1.012 - ETA: 0s - loss: 0.999 - ETA: 0s - loss: 1.012 - ETA: 0s - loss: 1.022 - ETA: 0s - loss: 1.016 - ETA: 0s - loss: 1.021 - ETA: 0s - loss: 1.025 - ETA: 0s - loss: 1.022 - ETA: 0s - loss: 1.019 - ETA: 0s - loss: 1.021 - 1s 136us/step - loss: 1.0244 - val_loss: 1.0421\n",
      "val weighted f1:  0.45706371191135736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15c23ee4d68>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x2,test_x2,train_y2,test_y2=train_test_split(docs_X,Y,test_size=0.1)\n",
    "model4.fit(train_x2,train_y2,validation_data=(test_x2,test_y2),epochs=20,callbacks=[metric1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So after applying all these different methods on the imbalanced dataset, we can conclude that the traditional ML algorithms like naive bayes,Linear SVM are working better than CNNs,LSTMS etc. The highest weighted F1 score obtained is for ANNs using tfidf vectors i.e 72.5%.Lets try it on balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4456, 2000)\n",
      "(1114, 2000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1114,)\n",
      "Test set prediction statistics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.79      0.87      0.83       347\n",
      "           2       0.00      0.00      0.00        36\n",
      "           3       0.00      0.00      0.00        40\n",
      "           4       0.25      0.04      0.07       125\n",
      "           5       0.76      0.94      0.84       566\n",
      "\n",
      "   micro avg       0.75      0.75      0.75      1114\n",
      "   macro avg       0.36      0.37      0.35      1114\n",
      "weighted avg       0.66      0.75      0.69      1114\n",
      "\n",
      "Train set prediction statistics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.90      0.95      0.93      1428\n",
      "           2       0.98      0.42      0.59       113\n",
      "           3       0.95      0.34      0.50       170\n",
      "           4       0.94      0.22      0.35       476\n",
      "           5       0.82      0.99      0.90      2269\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      4456\n",
      "   macro avg       0.92      0.58      0.65      4456\n",
      "weighted avg       0.87      0.85      0.82      4456\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "vec5=TfidfVectorizer()\n",
    "train_x5=vec.fit_transform(train_x)\n",
    "test_x5=vec.transform(test_x)\n",
    "print(train_x5.shape)\n",
    "print(test_x5.shape)\n",
    "lr=LogisticRegression()\n",
    "lr.fit(train_x5,train_y)\n",
    "predy_test=lr.predict(test_x5)\n",
    "print(predy_test.shape)\n",
    "print(\"Test set prediction statistics\")\n",
    "print(classification_report(test_y,predy_test))\n",
    "predy_train=lr.predict(train_x5)\n",
    "print(\"Train set prediction statistics\")\n",
    "print(classification_report(train_y,predy_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                         0\n",
       "App Version Code        1099\n",
       "App Version Name        1099\n",
       "Review Text                0\n",
       "Review Title               0\n",
       "Star Rating                0\n",
       "cleaned review text        0\n",
       "cleaned review title       0\n",
       "len title                  0\n",
       "len texts                  0\n",
       "tokens cleaned texts       0\n",
       "len tokens                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(df1.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11345, 2000)\n",
      "(1114, 2000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1114,)\n",
      "Test set prediction statistics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.79      0.80      0.79       347\n",
      "           2       0.05      0.11      0.07        36\n",
      "           3       0.11      0.20      0.14        40\n",
      "           4       0.20      0.41      0.27       125\n",
      "           5       0.85      0.54      0.66       566\n",
      "\n",
      "   micro avg       0.58      0.58      0.58      1114\n",
      "   macro avg       0.40      0.41      0.39      1114\n",
      "weighted avg       0.70      0.58      0.62      1114\n",
      "\n",
      "Train set prediction statistics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.89      0.79      0.84      2269\n",
      "           2       0.57      0.84      0.68      2269\n",
      "           3       0.52      0.35      0.42      2269\n",
      "           4       0.50      0.61      0.55      2269\n",
      "           5       0.81      0.63      0.71      2269\n",
      "\n",
      "   micro avg       0.64      0.64      0.64     11345\n",
      "   macro avg       0.66      0.64      0.64     11345\n",
      "weighted avg       0.66      0.64      0.64     11345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sm=SMOTE('not majority')\n",
    "train_x_sm,train_y_sm=sm.fit_sample(train_x5,train_y)\n",
    "print(train_x_sm.shape)\n",
    "print(test_x5.shape)\n",
    "lr=LogisticRegression()\n",
    "lr.fit(train_x_sm,train_y_sm)\n",
    "predy_test=lr.predict(test_x5)\n",
    "print(predy_test.shape)\n",
    "print(\"Test set prediction statistics\")\n",
    "print(classification_report(test_y,predy_test))\n",
    "predy_train=lr.predict(train_x_sm)\n",
    "print(\"Train set prediction statistics\")\n",
    "print(classification_report(train_y_sm,predy_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set prediction statistics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.82      0.78      0.80       347\n",
      "           2       0.07      0.14      0.09        36\n",
      "           3       0.08      0.17      0.11        40\n",
      "           4       0.18      0.37      0.24       125\n",
      "           5       0.86      0.56      0.68       566\n",
      "\n",
      "   micro avg       0.58      0.58      0.58      1114\n",
      "   macro avg       0.40      0.40      0.38      1114\n",
      "weighted avg       0.72      0.58      0.63      1114\n",
      "\n",
      "Train set prediction statistics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.90      0.87      0.89      2269\n",
      "           2       0.88      0.94      0.91      2269\n",
      "           3       0.85      0.77      0.81      2269\n",
      "           4       0.61      0.70      0.65      2269\n",
      "           5       0.71      0.65      0.68      2269\n",
      "\n",
      "   micro avg       0.79      0.79      0.79     11345\n",
      "   macro avg       0.79      0.79      0.79     11345\n",
      "weighted avg       0.79      0.79      0.79     11345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec3=TfidfVectorizer()\n",
    "train_x3=vec3.fit_transform(train_x)\n",
    "test_x3=vec3.transform(test_x)\n",
    "sm=SMOTE('not majority')\n",
    "train_x_sm,train_y_sm=sm.fit_sample(train_x3,train_y)\n",
    "clf3=MultinomialNB()\n",
    "clf3.fit(train_x_sm,train_y_sm)\n",
    "predy_test=clf3.predict(test_x3)\n",
    "predy_train=clf3.predict(train_x_sm)\n",
    "print(\"Test set prediction statistics\")\n",
    "print(classification_report(test_y,predy_test))\n",
    "print(\"Train set prediction statistics\")\n",
    "print(classification_report(train_y_sm,predy_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set prediction statistics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.79      0.88      0.83       347\n",
      "           2       0.06      0.06      0.06        36\n",
      "           3       0.10      0.10      0.10        40\n",
      "           4       0.18      0.37      0.24       125\n",
      "           5       0.82      0.58      0.68       566\n",
      "\n",
      "   micro avg       0.62      0.62      0.62      1114\n",
      "   macro avg       0.39      0.40      0.38      1114\n",
      "weighted avg       0.69      0.62      0.64      1114\n",
      "\n",
      "Train set prediction statistics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.41      0.84      0.56      2269\n",
      "           2       0.70      0.28      0.40      2269\n",
      "           3       0.67      0.18      0.28      2269\n",
      "           4       0.45      0.67      0.54      2269\n",
      "           5       0.76      0.63      0.69      2269\n",
      "\n",
      "   micro avg       0.52      0.52      0.52     11345\n",
      "   macro avg       0.60      0.52      0.49     11345\n",
      "weighted avg       0.60      0.52      0.49     11345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec1=CountVectorizer()\n",
    "train_x2=vec1.fit_transform(train_x)\n",
    "test_x2=vec1.transform(test_x)\n",
    "sm=SMOTE('not majority')\n",
    "train_x_sm,train_y_sm=sm.fit_sample(train_x2,train_y)\n",
    "clf2=MultinomialNB()\n",
    "clf2.fit(train_x_sm,train_y_sm)\n",
    "predy_test=clf2.predict(test_x2)\n",
    "predy_train=clf2.predict(train_x_sm)\n",
    "print(\"Test set prediction statistics\")\n",
    "print(classification_report(test_y,predy_test))\n",
    "print(\"Train set prediction statistics\")\n",
    "print(classification_report(train_y_sm,predy_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5=Sequential()\n",
    "model5.add(Dense(64,activation='relu',input_dim=len(vec1.vocabulary_)))\n",
    "model5.add(Dropout(0.2))\n",
    "model5.add(Dense(5,activation='softmax'))\n",
    "model5.compile(loss='categorical_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "140/139 [==============================] - ETA: 34s - loss: 1.62 - ETA: 4s - loss: 1.5880 - ETA: 2s - loss: 1.539 - ETA: 1s - loss: 1.499 - ETA: 1s - loss: 1.461 - ETA: 1s - loss: 1.412 - ETA: 0s - loss: 1.368 - ETA: 0s - loss: 1.342 - ETA: 0s - loss: 1.297 - ETA: 0s - loss: 1.267 - ETA: 0s - loss: 1.244 - ETA: 0s - loss: 1.213 - ETA: 0s - loss: 1.185 - ETA: 0s - loss: 1.159 - ETA: 0s - loss: 1.129 - ETA: 0s - loss: 1.121 - 1s 9ms/step - loss: 1.1170 - val_loss: 0.8414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val weighted f1:  0.6848157678189807\n",
      "Epoch 2/5\n",
      "140/139 [==============================] - ETA: 0s - loss: 1.121 - ETA: 0s - loss: 1.022 - ETA: 0s - loss: 0.879 - ETA: 0s - loss: 0.733 - ETA: 0s - loss: 1.174 - ETA: 0s - loss: 1.449 - ETA: 0s - loss: 1.548 - ETA: 0s - loss: 1.575 - ETA: 0s - loss: 1.565 - ETA: 0s - loss: 1.547 - ETA: 0s - loss: 1.524 - ETA: 0s - loss: 1.494 - ETA: 0s - loss: 1.581 - ETA: 0s - loss: 1.623 - ETA: 0s - loss: 1.644 - ETA: 0s - loss: 1.650 - ETA: 0s - loss: 1.647 - ETA: 0s - loss: 1.636 - 1s 8ms/step - loss: 1.6345 - val_loss: 1.4625\n",
      "val weighted f1:  0.4402413851590232\n",
      "Epoch 3/5\n",
      "140/139 [==============================] - ETA: 0s - loss: 1.331 - ETA: 0s - loss: 1.231 - ETA: 0s - loss: 1.150 - ETA: 0s - loss: 1.596 - ETA: 0s - loss: 1.775 - ETA: 0s - loss: 1.839 - ETA: 0s - loss: 1.804 - ETA: 0s - loss: 1.747 - ETA: 0s - loss: 1.669 - ETA: 0s - loss: 1.653 - ETA: 0s - loss: 1.614 - ETA: 0s - loss: 1.565 - ETA: 0s - loss: 1.520 - ETA: 0s - loss: 1.463 - ETA: 0s - loss: 1.419 - 1s 7ms/step - loss: 1.3924 - val_loss: 0.8605\n",
      "val weighted f1:  0.6965239722314741\n",
      "Epoch 4/5\n",
      "140/139 [==============================] - ETA: 2s - loss: 0.732 - ETA: 1s - loss: 0.747 - ETA: 1s - loss: 0.764 - ETA: 0s - loss: 0.761 - ETA: 0s - loss: 0.754 - ETA: 0s - loss: 0.739 - ETA: 0s - loss: 0.738 - ETA: 0s - loss: 0.719 - ETA: 0s - loss: 0.724 - ETA: 0s - loss: 0.738 - ETA: 0s - loss: 0.770 - ETA: 0s - loss: 0.769 - ETA: 0s - loss: 0.795 - ETA: 0s - loss: 0.865 - ETA: 0s - loss: 0.920 - ETA: 0s - loss: 0.952 - ETA: 0s - loss: 0.974 - 1s 7ms/step - loss: 0.9807 - val_loss: 1.1243\n",
      "val weighted f1:  0.6359707534299279\n",
      "Epoch 5/5\n",
      "140/139 [==============================] - ETA: 0s - loss: 1.128 - ETA: 1s - loss: 1.003 - ETA: 0s - loss: 0.941 - ETA: 0s - loss: 0.867 - ETA: 0s - loss: 0.972 - ETA: 0s - loss: 1.180 - ETA: 0s - loss: 1.288 - ETA: 0s - loss: 1.345 - ETA: 0s - loss: 1.344 - ETA: 0s - loss: 1.314 - ETA: 0s - loss: 1.263 - ETA: 0s - loss: 1.205 - ETA: 0s - loss: 1.312 - ETA: 0s - loss: 1.363 - ETA: 0s - loss: 1.371 - ETA: 0s - loss: 1.365 - ETA: 0s - loss: 1.347 - 1s 7ms/step - loss: 1.3300 - val_loss: 1.5446\n",
      "val weighted f1:  0.3265325111511511\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15c2a928c88>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y1=train_y_sm-1\n",
    "test_y1=test_y-1\n",
    "train_y_hotenc=to_categorical(train_y1)\n",
    "test_y_hotenc=to_categorical(test_y1)\n",
    "model5.fit_generator(generator=batch_generator(train_x_sm,train_y_hotenc, 32),\n",
    "                    epochs=5, validation_data=(test_x2, test_y_hotenc),steps_per_epoch=train_x3.shape[0]/32,callbacks=[metric1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After going through all different methods we can conclude that ANNs with TfidfVectorizer is giving the best weighted f1 score.That is we will be using model1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1412,)\n"
     ]
    }
   ],
   "source": [
    "test_final_x4=vec3.transform(df2['cleaned review text'])\n",
    "pred_test_final=model1.predict_generator(generator=batch_generator2(test_final_x4,32),steps=test_final_x4.shape[0]/32)\n",
    "pred_test_final=np.argmax(pred_test_final,axis=1)\n",
    "print(pred_test_final.shape)\n",
    "pred_test_final=pred_test_final+1\n",
    "df2['Star Rating']=pred_test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "df3=pd.read_csv(\"test.csv\")\n",
    "df3['Star Rating']=5*np.ones((df3.shape[0],1),dtype='int')\n",
    "for idx in range(df3.shape[0]):\n",
    "    id1=df3.ix[idx,'id']\n",
    "    v=df2[df2['id']==id1]['Star Rating']\n",
    "    if (len(v)!=0):\n",
    "        df3.ix[idx,'Star Rating']=v.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df=df3[['id','Star Rating']]\n",
    "sub_df.to_csv(\"Submission1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
